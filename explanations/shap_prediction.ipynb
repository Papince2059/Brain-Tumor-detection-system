{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanuj/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/tanuj/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# # 1. Load ResNet50 model\n",
    "# model = models.resnet50(pretrained=False)\n",
    "# model.fc = torch.nn.Linear(model.fc.in_features, 4)  # 4 classes\n",
    "# model.load_state_dict(torch.load(\"brain_tumor_model.pth\", map_location=torch.device('cpu')))\n",
    "# model.eval()\n",
    "\n",
    "# # 2. Class labels\n",
    "# class_names = ['Glioma','Meningioma','No Tumor', 'Pituitary']\n",
    "\n",
    "# # 3. Image preprocessing (ImageNet-style)\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],  # ImageNet stats\n",
    "#                          std=[0.229, 0.224, 0.225])\n",
    "# ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Load and preprocess the MRI image\n",
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image_tensor = transform(image).unsqueeze(0)  # Add batch dim\n",
    "    return image, image_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. SHAP uses background data â€“ select 5 random background images\n",
    "def get_background_images(folder, num=5):\n",
    "    images = []\n",
    "    for fname in os.listdir(folder)[:num]:\n",
    "        path = os.path.join(folder, fname)\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        images.append(transform(img))\n",
    "    return torch.stack(images)\n",
    "\n",
    "# 6. Run SHAP explanation\n",
    "def explain_with_shap(image_path, background_folder):\n",
    "    pil_img, input_tensor = load_image(image_path)\n",
    "\n",
    "    # Get background data\n",
    "    background = get_background_images(background_folder)\n",
    "\n",
    "    # Use GradientExplainer for PyTorch models\n",
    "    explainer = shap.GradientExplainer(model, background)\n",
    "    shap_values, indexes = explainer.shap_values(input_tensor)\n",
    "\n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probs = torch.nn.functional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
