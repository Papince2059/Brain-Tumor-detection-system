{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brain Tumor Classification and Explainability Pipeline\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "import shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data Transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataset and DataLoader\n",
    "data_dir = \"C:\\Users\\Papince Gupta\\OneDrive\\Desktop\\semester 6\\SE\\project\\Nidaan_AI\\brain_tumor_mri_dataset\"\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'Training'), transform=transform)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(data_dir, 'Testing'), transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanuj/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/tanuj/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/tanuj/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:12<00:00, 8.31MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Model Initialization\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 4)  # 4 classes\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Training Pipeline\n",
    "def train(model, train_loader, val_loader, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "    print('Training complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Explanation using LIME\n",
    "def explain_with_lime(model, image, class_names):\n",
    "    explainer = lime_image.LimeImageExplainer()\n",
    "    \n",
    "    def predict(input):\n",
    "        input = torch.tensor(input.transpose((0, 3, 1, 2)), dtype=torch.float32).to(device)\n",
    "        outputs = model(input)\n",
    "        return torch.nn.functional.softmax(outputs, dim=1).detach().cpu().numpy()\n",
    "\n",
    "    explanation = explainer.explain_instance(\n",
    "        image.astype('double'), predict, top_labels=4, hide_color=0\n",
    "    )\n",
    "\n",
    "    image, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, hide_rest=False)\n",
    "    plt.imshow(mark_boundaries(image, mask))\n",
    "    plt.title(f'Explanation for {class_names[explanation.top_labels[0]]}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Explanation using Grad-CAM\n",
    "def explain_with_gradcam(model, image, target_layer, class_idx):\n",
    "    cam = GradCAM(model=model, target_layers=[target_layer])\n",
    "    grayscale_cam = cam(input_tensor=image.unsqueeze(0).to(device), targets=[ClassifierOutputTarget(class_idx)])[0]\n",
    "    visualization = show_cam_on_image(image.permute(1, 2, 0).cpu().numpy(), grayscale_cam, use_rgb=True)\n",
    "    plt.imshow(visualization)\n",
    "    plt.title(f'Grad-CAM for class {class_idx}')\n",
    "    plt.show()\n",
    "\n",
    "# Explanation using SHAP\n",
    "def explain_with_shap(model, image, background):\n",
    "    explainer = shap.GradientExplainer(model, background)\n",
    "    shap_values = explainer.shap_values(image.unsqueeze(0).to(device))\n",
    "    shap.image_plot(shap_values, image.unsqueeze(0).cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    train(model, train_loader, val_loader, epochs=10)\n",
    "    torch.save(model.state_dict(), \"brain_tumor_model.pth\")\n",
    "    print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tanuj/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/tanuj/anaconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Brain Tumor Classification and Explainability Pipeline\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "import shap\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# Device Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Data Transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset and DataLoader\n",
    "data_dir = \"C:\\Users\\Papince Gupta\\OneDrive\\Desktop\\semester 6\\SE\\project\\Nidaan_AI\\brain_tumor_mri_dataset\"\n",
    "train_dataset = datasets.ImageFolder(os.path.join(data_dir, 'Training'), transform=transform)\n",
    "val_dataset = datasets.ImageFolder(os.path.join(data_dir, 'Testing'), transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Model Initialization\n",
    "model = models.resnet50(pretrained=True)\n",
    "model.fc = nn.Linear(model.fc.in_features, 4)  # 4 classes\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
    "\n",
    "# Training Pipeline\n",
    "def train(model, train_loader, val_loader, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "    print('Training complete.')\n",
    "\n",
    "# Explanation using LIME\n",
    "def explain_with_lime(model, image, class_names):\n",
    "    explainer = lime_image.LimeImageExplainer()\n",
    "    \n",
    "    def predict(input):\n",
    "        input = torch.tensor(input.transpose((0, 3, 1, 2)), dtype=torch.float32).to(device)\n",
    "        outputs = model(input)\n",
    "        return torch.nn.functional.softmax(outputs, dim=1).detach().cpu().numpy()\n",
    "\n",
    "    explanation = explainer.explain_instance(\n",
    "        image.astype('double'), predict, top_labels=4, hide_color=0\n",
    "    )\n",
    "\n",
    "    image, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, hide_rest=False)\n",
    "    plt.imshow(mark_boundaries(image, mask))\n",
    "    plt.title(f'Explanation for {class_names[explanation.top_labels[0]]}')\n",
    "    plt.show()\n",
    "\n",
    "# Explanation using Grad-CAM\n",
    "def explain_with_gradcam(model, image, target_layer, class_idx):\n",
    "    cam = GradCAM(model=model, target_layers=[target_layer])\n",
    "    grayscale_cam = cam(input_tensor=image.unsqueeze(0).to(device), targets=[ClassifierOutputTarget(class_idx)])[0]\n",
    "    visualization = show_cam_on_image(image.permute(1, 2, 0).cpu().numpy(), grayscale_cam, use_rgb=True)\n",
    "    plt.imshow(visualization)\n",
    "    plt.title(f'Grad-CAM for class {class_idx}')\n",
    "    plt.show()\n",
    "\n",
    "# Explanation using SHAP\n",
    "def explain_with_shap(model, image, background):\n",
    "    explainer = shap.GradientExplainer(model, background)\n",
    "    shap_values = explainer.shap_values(image.unsqueeze(0).to(device))\n",
    "    shap.image_plot(shap_values, image.unsqueeze(0).cpu().numpy())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 0.3848, Val Loss: 1.0328\n",
      "Epoch [2/10], Train Loss: 0.2016, Val Loss: 0.7852\n",
      "Epoch [3/10], Train Loss: 0.1648, Val Loss: 9.2042\n",
      "Epoch [4/10], Train Loss: 0.1768, Val Loss: 0.5896\n",
      "Epoch [5/10], Train Loss: 0.0821, Val Loss: 0.4599\n",
      "Epoch [6/10], Train Loss: 0.0900, Val Loss: 0.6548\n",
      "Epoch [7/10], Train Loss: 0.0693, Val Loss: 0.7901\n",
      "Epoch [8/10], Train Loss: 0.0647, Val Loss: 1.3631\n",
      "Epoch [9/10], Train Loss: 0.0735, Val Loss: 1.2536\n",
      "Epoch [10/10], Train Loss: 0.0199, Val Loss: 0.5158\n",
      "Training complete.\n",
      "Model saved successfully in both .pth and .pkl formats.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train(model, train_loader, val_loader, epochs=10)\n",
    "    torch.save(model.state_dict(), \"brain_tumor_model.pth\")\n",
    "    with open(\"brain_tumor_model.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model.state_dict(), f)\n",
    "    print(\"Model saved successfully in both .pth and .pkl formats.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
